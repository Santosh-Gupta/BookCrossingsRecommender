{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "BookCrossingsTraining.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "[View in Colaboratory](https://colab.research.google.com/github/Santosh-Gupta/BookCrossingsRecommender/blob/master/BookCrossingsTraining.ipynb)"
      ]
    },
    {
      "metadata": {
        "id": "XX7iWmS1FEIh",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Now that we have the refined data and book info files produced in the BookCrossingsDataAnalysis.ipynb notebook, we can train models on the data. "
      ]
    },
    {
      "metadata": {
        "id": "cz8FsZRH99zd",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import re\n",
        "from collections import Counter\n",
        "import time\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "from __future__ import print_function\n",
        "import collections\n",
        "import math\n",
        "import numpy as np\n",
        "import random\n",
        "import zipfile\n",
        "from six.moves import range\n",
        "from six.moves.urllib.request import urlretrieve\n",
        "\n",
        "import os\n",
        "\n",
        "from urllib.request import urlretrieve\n",
        "from os.path import isfile, isdir\n",
        "import zipfile\n",
        "import tensorflow as tf\n",
        "from matplotlib import pylab\n",
        "from six.moves import range\n",
        "from six.moves.urllib.request import urlretrieve\n",
        "from sklearn.manifold import TSNE\n",
        "import pandas as pd\n",
        "\n",
        "!pip install -U -q PyDrive\n",
        "\n",
        "from google.colab import files\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "import zipfile\n",
        "import os\n",
        "import sys\n",
        "from numpy import genfromtxt\n",
        "\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)\n",
        "\n",
        "vocabulary_size =  40988 #number of books\n",
        "\n",
        "tf.logging.set_verbosity(tf.logging.ERROR)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "edaH1P6ZDzSz",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Input Google drive file ID for the refined ratings file produced in BookCrossingsDataAnalysis.ipynb notebook"
      ]
    },
    {
      "metadata": {
        "id": "Jkf7DrmN-Jmv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 328
        },
        "outputId": "563b648e-c04d-41de-c7e0-139faf6bc5b5"
      },
      "cell_type": "code",
      "source": [
        "dl_id = input(\"Enter Gdrive file ID for Refined Ratings: \") #Book Crossing 1mtg_TdC4FBlbQh30Yt1-Rxcyu2Q38Axv\n",
        "\n",
        "\n",
        "# DOWNLOAD ZIP\n",
        "print (\"Downloading  file\")\n",
        "myDownload = drive.CreateFile({'id': dl_id})\n",
        "myDownload.GetContentFile('DlRefinedRatings.csv')\n",
        "print( os.listdir() )\n",
        "\n",
        "my_data = genfromtxt('DlRefinedRatings.csv', delimiter=',' ,  dtype=int, skip_header=1)\n",
        "\n",
        "print(my_data[0:15])\n"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Enter Gdrive file ID for Refined Ratings: 1mtg_TdC4FBlbQh30Yt1-Rxcyu2Q38Axv\n",
            "Downloading  file\n",
            "['datalab', '.config', '.local', '.ipython', 'DlRefinedRatings.csv', '.keras', '.forever', '.cache']\n",
            "[[1082427  259004     784       8      22      54      17]\n",
            " [1082450  259004   24422       7      22      91      -1]\n",
            " [1082415  259004    3085       7      22     183      -1]\n",
            " [1082363  259004   24458       6      22     250      -1]\n",
            " [1082357  259004    2465       6      22     251      -1]\n",
            " [1082426  259004    6411       8      22     258      -1]\n",
            " [1082425  259004   14199       8      22     260      -1]\n",
            " [1082422  259004    5035      10      22     264      -1]\n",
            " [1082391  259004    3447       9      22     267      -1]\n",
            " [1082412  259004    2483       9      22     281      -1]\n",
            " [1082451  259004   24457       8      22     304      -1]\n",
            " [1082440  259004   13666       6      22     352      -1]\n",
            " [1082378  259004    3217       7      22     414      -1]\n",
            " [1082361  259004    5370       7      22     419      -1]\n",
            " [1082452  259004   24405       7      22     430      -1]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "IMCH6OtmEYRi",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Input Google drive file ID for the book info file produced in BookCrossingsDataAnalysis.ipynb notebook"
      ]
    },
    {
      "metadata": {
        "id": "z2hAjeu-aiDX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "outputId": "4046893a-e59e-4170-d722-4f6cf6950a3b"
      },
      "cell_type": "code",
      "source": [
        "dl_id = input(\"Enter Gdrive file ID for Book info: \") #Book Crossing  1thtwqN1fhyiLeEnmj51ZhEaW5DgonYgC\n",
        "\n",
        "\n",
        "# DOWNLOAD ZIP\n",
        "print (\"Downloading  file\")\n",
        "myDownload = drive.CreateFile({'id': dl_id})\n",
        "myDownload.GetContentFile('BookInfo.csv')\n",
        "print( os.listdir() )"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Enter Gdrive file ID for Book info: 1thtwqN1fhyiLeEnmj51ZhEaW5DgonYgC\n",
            "Downloading  file\n",
            "['datalab', '.config', '.local', '.ipython', 'BookInfo.csv', 'DlRefinedRatings.csv', '.keras', '.forever', '.cache']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "whuTVyD5ElzA",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Create dictionaries to look up Authors and Book Titles from the EmbedID"
      ]
    },
    {
      "metadata": {
        "id": "LejKSDZ5LQEQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "ebb482da-3199-49b1-aafb-b8fac4702136"
      },
      "cell_type": "code",
      "source": [
        "b = pd.read_csv( 'BookInfo.csv' )\n",
        "b.head(30)\n",
        "bookDictionary = b.set_index('EmbedID').to_dict()['Book_Title']\n",
        "bookDictionary[7]\n",
        "authorDictionary = b.set_index('EmbedID').to_dict()['Author']\n",
        "authorDictionary[7]"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Dan Brown'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "metadata": {
        "id": "7nkPzE95FXfX",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "This code section produces a batch of input data and corresponding labels. "
      ]
    },
    {
      "metadata": {
        "id": "NE2HW6llJZtC",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "data_index = 0\n",
        "epoch_index = 0\n",
        "recEpoch_indexA = 0 #Used to help keep store of the total number of epoches with the models\n",
        "\n",
        "def generate_batch(batch_size, inputCount): \n",
        "  \n",
        "    global data_index, epoch_index\n",
        "    \n",
        "    span = inputCount \n",
        "    batch = np.ndarray(shape=(batch_size, span), dtype=np.int32) \n",
        "    labels = np.ndarray(shape=(batch_size, 1), dtype=np.int32)\n",
        "    \n",
        "    \n",
        "    userbracketDF = my_data[data_index: data_index+ my_data[data_index, 6], 2 ]\n",
        "    headernumber = my_data[:,6]\n",
        "    assert headernumber[data_index] != -1, 'data_index in incorrect spot. Most likely fix is to reset data_index to 0'\n",
        "        \n",
        "    n=0\n",
        "    while n < batch_size:\n",
        "      b=0\n",
        "      while b < len(userbracketDF) and n < batch_size :\n",
        "        #print('n is', n)\n",
        "        if b== 0:\n",
        "          potentialBatch=userbracketDF[b+1:len(userbracketDF)]\n",
        "        else:\n",
        "          firsthalf = userbracketDF[0:b]\n",
        "          secondhalf = userbracketDF[b+1:len(userbracketDF)]\n",
        "          potentialBatch = np.concatenate((firsthalf, secondhalf), axis=0)\n",
        "        potentialBatchSet = set(potentialBatch)\n",
        "        if inputCount > len(potentialBatchSet):\n",
        "          print('potentialBatchSet is ', potentialBatchSet)\n",
        "          print('data_index is ', data_index)\n",
        "          raise Exception('Dataset is for a smaller Cbow Window, data needs to be checked') \n",
        "        batch[n] = random.sample(potentialBatchSet,  inputCount)\n",
        "        labels[n,0]=userbracketDF[b]\n",
        "        n=n+1\n",
        "        b=b+1\n",
        "      \n",
        "      data_index = (data_index + headernumber[data_index] ) % len(headernumber)\n",
        "      \n",
        "      if data_index == 0:\n",
        "        epoch_index = epoch_index + 1\n",
        "        print('Completed %d Epochs' % epoch_index)\n",
        "      \n",
        "      #print('data_index is ', data_index)\n",
        "      assert headernumber[data_index] != -1, 'data_index in incorrect spot'\n",
        "      userbracketDF = my_data[data_index: data_index+ my_data[data_index, 6], 2 ]\n",
        "\n",
        "    return batch, labels     \n",
        "      \n",
        "# here, goes = generate_batch(20, 1) # to do next, insert %len(headernumber)\n",
        "# print('batch', here)\n",
        "# print('labels', goes)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "q39byp4PGDLV",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Define the graph"
      ]
    },
    {
      "metadata": {
        "id": "EN8lEIKJxmRF",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "batch_size = 128 #2^8\n",
        "\n",
        "embedding_size = 200 # 2^8 Dimension of the embedding vector.\n",
        "num_inputs =1\n",
        "#skip_window = 1 # How many words to consider left and right.\n",
        "#num_skips = 2 # How many times to reuse an input to generate a label.\n",
        "                      \n",
        "# We pick a random validation set to sample nearest neighbors. here we limit the\n",
        "# validation samples to the words that have a low numeric ID, which by\n",
        "# construction are also the most frequent. \n",
        "valid_size = 60 # Random set of words to evaluate similarity on.\n",
        "valid_window = 5000 # Only pick dev samples in the head of the distribution.\n",
        "#valid_examples = np.array(random.sample(range(valid_window), valid_size))\n",
        "# Random set of words to evaluate similarity on.\n",
        "num_sampled = 65 # Number of negative examples to sample.\n",
        "\n",
        "graph = tf.Graph()\n",
        "\n",
        "with graph.as_default(): #took out \" , tf.device('/cpu:0')\"\n",
        " \n",
        "  valid_examples = np.array(random.sample(range(1, valid_window), valid_size)) #put inside graph to get new words each time\n",
        "  #for same words each time keep outside graph\n",
        "  \n",
        "  # Input data.\n",
        "  train_dataset = tf.placeholder(tf.int32, shape=[batch_size, num_inputs ])\n",
        "  train_labels = tf.placeholder(tf.int32, shape=[batch_size, 1])\n",
        "  valid_dataset = tf.constant(valid_examples, dtype=tf.int32)\n",
        "  valid_datasetSM = tf.constant(valid_examples, dtype=tf.int32)\n",
        "\n",
        "  epochCount = tf.get_variable( 'epochCount', initializer= 0) #to store epoch count to total # of epochs are known\n",
        "  update_epoch = tf.assign(epochCount, epochCount + 1)\n",
        "  # Variables.\n",
        "  #embeddings = tf.Variable(\n",
        "  #  tf.random_uniform([vocabulary_size, embedding_size], -1.0, 1.0))\n",
        "  embeddings = tf.get_variable( 'embeddings', \n",
        "    initializer= tf.random_uniform([vocabulary_size, embedding_size], -1.0, 1.0))\n",
        "  #softmax_weights = tf.Variable(\n",
        "  #  tf.truncated_normal([vocabulary_size, embedding_size],\n",
        "  #                       stddev=1.0 / math.sqrt(embedding_size)))\n",
        "  softmax_weights = tf.get_variable( 'softmax_weights',\n",
        "    initializer= tf.truncated_normal([vocabulary_size, embedding_size],\n",
        "                         stddev=1.0 / math.sqrt(embedding_size)))\n",
        "  \n",
        "  softmax_biases = tf.get_variable('softmax_biases', \n",
        "    initializer= tf.zeros([vocabulary_size]),  trainable=False )\n",
        "  #Made softmax biases untrainable because they do not seem to give significant\n",
        "  #improvement. Also allows for a 2nd set of embeddings to analyze. \n",
        "  #https://stats.stackexchange.com/questions/249565/word2vec-neural-network-bias-units/250464\n",
        "  #Both Richard Sorcher and Andrew Ng's NLP lectures do not contain a bias term\n",
        "  #in their equations. \n",
        "  \n",
        "  # Model.\n",
        "  # Look up embeddings for inputs.\n",
        "  embed = tf.nn.embedding_lookup(embeddings, train_dataset) #train data set is\n",
        "  #the center word for skip-gram mode. So we're looking up the vector for that\n",
        "  #center word. Gets the embeddings for every word in the batch\n",
        "  # Compute the softmax loss, using a sample of the negative labels each time.\n",
        "  embed_reshaped = tf.reshape( embed, [batch_size*num_inputs, embedding_size] )\n",
        "  \n",
        "  \n",
        "  segments= np.arange(batch_size).repeat(num_inputs)\n",
        "  #segments= np.arange(embedding_size).repeat(num_inputs)\n",
        "  #should be batch size, not embedding size\n",
        "  \n",
        "  averaged_embeds = tf.segment_mean(embed_reshaped, segments, name=None)\n",
        "  \n",
        "  #def loss_function():\n",
        "    #return tf.reduce_mean( tf.nn.sampled_softmax_loss(weights=softmax_weights, biases=softmax_biases, inputs=averaged_embeds,\n",
        "                               #labels=train_labels, num_sampled=num_sampled, num_classes=vocabulary_size))\n",
        "  \n",
        "  loss = tf.reduce_mean(\n",
        "    tf.nn.sampled_softmax_loss(weights=softmax_weights, biases=softmax_biases, inputs=averaged_embeds,\n",
        "                               labels=train_labels, num_sampled=num_sampled, num_classes=vocabulary_size))\n",
        "            #fun fact remove_accidental_hits: A bool. whether to remove \"accidental hits\" where \n",
        "            #a sampled class equals one of the target classes. Default is True.\n",
        "\n",
        "  # Optimizer.\n",
        "  # Note: The optimizer will optimize the softmax_weights AND the embeddings.\n",
        "  # This is because the embeddings are defined as a variable quantity and the\n",
        "  # optimizer's `minimize` method will by default modify all variable quantities \n",
        "  # that contribute to the tensor it is passed.\n",
        "  # See docs on `tf.train.Optimizer.minimize()` for more details.\n",
        "  optimizer = tf.train.AdagradOptimizer(0.03).minimize(loss) #Original learning rate was 1.0\n",
        "  \n",
        "  # Compute the similarity between minibatch examples and all embeddings.\n",
        "  # We use the cosine distance:\n",
        "  norm = tf.sqrt(tf.reduce_sum(tf.square(embeddings), 1, keepdims=True))\n",
        "  normSM = tf.sqrt(tf.reduce_sum(tf.square(softmax_weights), 1, keepdims=True))\n",
        "  \n",
        "  normalized_embeddings = embeddings / norm\n",
        "  normalized_embeddingsSM = softmax_weights / normSM\n",
        "  \n",
        "  valid_embeddings = tf.nn.embedding_lookup(\n",
        "    normalized_embeddings, valid_dataset) #A bunch of normalized embeddings, randomly selected when valid_examples was initilized. \n",
        "  valid_embeddingsSM = tf.nn.embedding_lookup(\n",
        "    normalized_embeddingsSM, valid_datasetSM)\n",
        "  \n",
        "  similarity = tf.matmul(valid_embeddings, tf.transpose(normalized_embeddings)) #multiplies each normalized embeds from valid_examples with all \n",
        "                                                                                                                        #other normalized embeds. Used to find most similiar embeds later\n",
        "  similaritySM = tf.matmul(valid_embeddingsSM, tf.transpose(normalized_embeddingsSM)) \n",
        "  #important to note that to note that throughout all these matrix operations, indices of the original embedding and SM matrices are kept in tact\n",
        "  #this is important for when we want to match an embedding with it's corresponding book, since the book dictionary uses the index as the key\n",
        "                                                                                                                              \n",
        "  saver = tf.train.Saver()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Udu7tyvOb8qs",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "This zip folder is used to save the checkpoints and zip them into a zipfile so that they can be easily uploaded to Google Drive"
      ]
    },
    {
      "metadata": {
        "id": "jcf12Y-4x8OL",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def zipfolder(foldername, target_dir):            \n",
        "    zipobj = zipfile.ZipFile(foldername + '.zip', 'w', zipfile.ZIP_DEFLATED)\n",
        "    rootlen = len(target_dir) + 1\n",
        "    for base, dirs, files in os.walk(target_dir):\n",
        "        for file in files:\n",
        "            fn = os.path.join(base, file)\n",
        "            zipobj.write(fn, fn[rootlen:])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "_h43CRbcGLVl",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "If you would like to continue from a previous Tensorflow checkpoint, select 'y' and put the Google file ID for the checkpoint"
      ]
    },
    {
      "metadata": {
        "id": "aDigURkRF1TS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        },
        "outputId": "b4c116db-3eb7-4364-b091-5ae06dbd22b7"
      },
      "cell_type": "code",
      "source": [
        "loadModel = input(\"Would you like to load a checkpoint? Type y or n: \") \n",
        "\n",
        "if loadModel == 'y':\n",
        "  auth.authenticate_user()\n",
        "  gauth = GoogleAuth()\n",
        "  gauth.credentials = GoogleCredentials.get_application_default()\n",
        "  drive = GoogleDrive(gauth)\n",
        "  \n",
        "  zip_id = input(\"Enter Gdrive file ID for tensorflow models: \") \n",
        "\n",
        "  if not os.path.exists('checkpointsBook2VecCbowWindow1Downloaded'):\n",
        "      os.makedirs('checkpointsBook2VecCbowWindow1Downloaded')\n",
        "\n",
        "  # DOWNLOAD ZIP\n",
        "  print (\"Downloading zip file\")\n",
        "  myzip = drive.CreateFile({'id': zip_id})\n",
        "  myzip.GetContentFile('model.zip')\n",
        "\n",
        "  # UNZIP ZIP\n",
        "  print (\"Uncompressing zip file\")\n",
        "  zip_ref = zipfile.ZipFile('model.zip', 'r')\n",
        "  zip_ref.extractall('checkpointsBook2VecCbowWindow1Downloaded/')\n",
        "  zip_ref.close()\n",
        "\n",
        "  print( os.getcwd() )\n",
        "  print( os.listdir('./checkpointsBook2VecCbowWindow1Downloaded') )\n",
        "  \n",
        "  #with tf.Session(graph=graph) as session:\n",
        "    #saver = tf.train.import_meta_graph('./checkpointsBook2VecCbowWindow3Downloaded/bookVec.ckpt.meta')\n",
        "    #saver.restore(session, './checkpointsBook2VecCbowWindow3Downloaded/bookVec.ckpt' )"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Would you like to load a checkpoint? Type y or n: y\n",
            "Enter Gdrive file ID for tensorflow models: 1DjPW4rpMP5Dl_7IQgTI_5NkzH2ResTeD\n",
            "Downloading zip file\n",
            "Uncompressing zip file\n",
            "/content\n",
            "['bookVec.ckpt.data-00000-of-00001', 'checkpoint', 'bookVec.ckpt.index', 'bookVec.ckpt.meta']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "BDq2Lj8AGf8t",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Run the Tensorflow graph in a session."
      ]
    },
    {
      "metadata": {
        "id": "jn9Zvv4PxtXV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 449
        },
        "outputId": "27a957e6-0a52-4c2c-b34e-5baf13da3054"
      },
      "cell_type": "code",
      "source": [
        "num_steps = 1000000\n",
        "\n",
        "if 'loadModel' not in locals() and 'loadModel' not in globals():\n",
        "  loadModel = 'n'\n",
        "\n",
        "uploadModel = drive.CreateFile() #used to upload checkpoints when graph is run\n",
        "\n",
        "with tf.Session(graph=graph) as session:\n",
        "  \n",
        "  if loadModel == 'y':\n",
        "    saver.restore(session, './checkpointsBook2VecCbowWindow1Downloaded/bookVec.ckpt' )\n",
        "  else:  \n",
        "    tf.global_variables_initializer().run() #Don't initalize variables after a checkpoint has been restored\n",
        "  \n",
        "  print('Initialized')\n",
        "  average_loss = 0\n",
        "  saveIteration = 1\n",
        "  for step in range(1, num_steps):\n",
        "    \n",
        "    batch_data, batch_labels = generate_batch(\n",
        "      batch_size, num_inputs)\n",
        "    #print(batch_data)\n",
        "    feed_dict = {train_dataset : batch_data, train_labels : batch_labels}\n",
        "    #graph_batch = session.run([train_dataset], feed_dict=feed_dict) \n",
        "    #print(graph_batch)\n",
        "    _, l = session.run([optimizer, loss], feed_dict=feed_dict) \n",
        "\n",
        "    average_loss += l\n",
        "    if step % 50000 == 0:\n",
        "      if step > 0:\n",
        "        average_loss = average_loss / 50000\n",
        "      # The average loss is an estimate of the loss over the last 2000 batches.\n",
        "      print('Average loss at step %d: %f' % (step, average_loss))\n",
        "      average_loss = 0\n",
        "    # note that this is expensive (~20% slowdown if computed every 500 steps)\n",
        "    \n",
        "    if step % 100000 == 0:\n",
        "      sim = similarity.eval()\n",
        "      for i in range(valid_size):\n",
        "        valid_word = bookDictionary[valid_examples[i]]\n",
        "        top_k = 8 # number of nearest neighbors\n",
        "        nearest = (-sim[i, :]).argsort()[1:top_k+1] #Returns index of most similiar values. Starts from 1 because index 0 will just be the same book\n",
        "        log = 'Nearest to %s : | ' % valid_word\n",
        "        for k in range(top_k):\n",
        "          close_word = bookDictionary[nearest[k]]\n",
        "          log = '%s %s | ' % (log, close_word) #Prints out most similiar books, in descending order from most to least similiar\n",
        "        print(log)\n",
        "        if gauth.credentials is None:\n",
        "        # Authenticate if they're not there\n",
        "          gauth.LocalWebserverAuth()\n",
        "        elif gauth.access_token_expired:\n",
        "          # Refresh them if expired\n",
        "          print( \"Google Drive Token Expired, Refreshing\")\n",
        "          gauth.Refresh()\n",
        "        else:\n",
        "          # Initialize the saved creds\n",
        "          gauth.Authorize()\n",
        "      \n",
        "    if step % 150000 == 0:\n",
        "      recEpoch_indexA =  epoch_index - recEpoch_indexA #how much did the epoch_index since it was last checked\n",
        "      #epochCount = tf.add(  epochCount, recEpoch_indexA, name=None )\n",
        "      for nE in range(0, recEpoch_indexA ):\n",
        "        session.run(update_epoch) #session run calls tend to be huge bottlenecks, keep in mind while determining the frequency\n",
        "      recEpoch_indexA = epoch_index\n",
        "      print('recEpoch_indexA is', recEpoch_indexA)\n",
        "      print( 'epochCount.eval() is ', epochCount.eval() )\n",
        "      print('epoch_index is ' , epoch_index)\n",
        "      \n",
        "      save_path = saver.save(session, \"B2VBookCrossingsckpt/bookVec.ckpt\") #Save checkpoint\n",
        "      \n",
        "      auth.authenticate_user()\n",
        "      gauth = GoogleAuth() #Gdrive authenticion code placed here since it expires after some time\n",
        "      gauth.credentials = GoogleCredentials.get_application_default()\n",
        "      drive = GoogleDrive(gauth) \n",
        "      uploadModel = drive.CreateFile() #Need to also create drive object with updated authenticion\n",
        "      \n",
        "      chptName = 'B2VBookCrossingsckpt'+str(saveIteration)\n",
        "      zipfolder(chptName, 'B2VBookCrossingsckpt') #2nd input is the folder name of the save path\n",
        "      uploadModel.SetContentFile(chptName+\".zip\")\n",
        "      uploadModel.Upload()\n",
        "      \n",
        "      print(\"Checkpoint uploaded to Google Drive\")\n",
        "      saveIteration += 1\n",
        "      os.remove(chptName+\".zip\") #Remove checkpoint zip file after upload\n",
        "\n",
        "                \n",
        "  #extract the embeddings\n",
        "  final_embeddings = normalized_embeddings.eval() #extract the embeddings\n",
        "  final_embeddingsSM = normalized_embeddingsSM.eval()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Initialized\n",
            "Completed 1 Epochs\n",
            "Completed 2 Epochs\n",
            "Completed 4 Epochs\n",
            "Completed 5 Epochs\n",
            "Completed 6 Epochs\n",
            "Completed 7 Epochs\n",
            "Completed 8 Epochs\n",
            "Completed 9 Epochs\n",
            "Completed 10 Epochs\n",
            "Completed 11 Epochs\n",
            "Completed 12 Epochs\n",
            "Completed 13 Epochs\n",
            "Completed 14 Epochs\n",
            "Completed 15 Epochs\n",
            "Completed 16 Epochs\n",
            "Completed 17 Epochs\n",
            "Completed 18 Epochs\n",
            "Completed 19 Epochs\n",
            "Completed 20 Epochs\n",
            "Completed 21 Epochs\n",
            "Completed 22 Epochs\n",
            "Completed 23 Epochs\n",
            "Completed 24 Epochs\n",
            "Completed 25 Epochs\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "NLeZW5hiGxKt",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The trained embeddings are analyzed in much more detail in the BookCrossingsAnalyzeEmbeddings.ipynb file. "
      ]
    }
  ]
}